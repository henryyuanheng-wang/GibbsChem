{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample grid for training data (code in neural.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Chempy.parameter import ModelParameters\n",
    "from Chempy.cem_function import posterior_function_returning_predictions\n",
    "from scipy.stats import norm as gaussian\n",
    "import os\n",
    "a = ModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This calculates a list of 5 trial values for each parameter around the prior value, as an array of 6 lists which will be combined\n",
    "# Set the desired Gaussian sigma values in the widths parameter (values > prior sigma are used to fully explore parameter space)\n",
    "# Parameter values are chosen that are evenly distributed in the Gaussian probability space (e.g. 16.7, 33, 50 etc. percentile points)\n",
    "\n",
    "N = a.training_size # No. data points per parameter\n",
    "widths = a.training_widths # Gaussian widths for parameters\n",
    "\n",
    "# Create 1d grid of data points equally spaced in probability space \n",
    "prob = np.linspace(1/(N+1), 1-1/(N+1), N)\n",
    "grids = [gaussian.ppf(prob) for _ in range(N+1)] # Normalize to unit Gaussian\n",
    "norm_grid = np.array(np.meshgrid(*grids)).T.reshape(-1,N+1)\n",
    "\n",
    "# Create grid in parameter space\n",
    "param_grid = [item*widths+a.p0 for item in norm_grid]\n",
    "\n",
    "# Save grids\n",
    "directory = 'Neural/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "np.save(directory+'training_norm_grid.npy',norm_grid)\n",
    "np.save(directory+'training_param_grid.npy',param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create abundance output\n",
    "param_grid = param_grid[:10] # For testing\n",
    "training_abundances = []\n",
    "for i,item in enumerate(param_grid):\n",
    "    abundances,_ = posterior_function_returning_predictions((item,a))\n",
    "    training_abundances.append(abundances)\n",
    "    if i%100 == 0:\n",
    "        print(\"Calculating abundance set %d of %d\" %(i,len(param_grid)))\n",
    "              \n",
    "# Save abundance table\n",
    "np.save('Neural/training_abundances.npy', training_abundances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTES\n",
    "\n",
    "- All code is now in neural.py file\n",
    "- Should put changeable parameters e.g. number of choices for each parameter in parameter.py file - DONE\n",
    "-  Find nicer way of using all rows from grid - DONE\n",
    "- Check whether to use Karakas 10 or Karakas 16 - Karakas 10 for testing\n",
    "- Automate number of traceable elements - just copy that from code - DONE\n",
    "\n",
    "*This may be a useful reference https://arxiv.org/abs/1502.01852, for recommendation of ReLU units, with w = np.random.randn(n) x np.sqrt(2/n) for initialized weights, from Stanford course*\n",
    "\n",
    "*Another useful reference: https://arxiv.org/abs/1412.6980 for 'Adam' learning method viz Stanford course*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Verification / Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ModelParameters()\n",
    "names = ['verif','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,name in enumerate(names): # Create two identically distributed datasets\n",
    "    length = a.verif_test_sizes[i]\n",
    "    param_grid = []\n",
    "    # Distribute data with prior widths\n",
    "    for _ in range(length):\n",
    "        param_grid.append(np.random.normal(size = len(a.p0), loc = a.p0,\n",
    "                                           scale = a.test_widths))\n",
    "    np.save(\"Neural/\"+name+\"_param_grid.npy\",param_grid)\n",
    "    \n",
    "    model_abundances = []\n",
    "    for j,jtem in enumerate(param_grid):\n",
    "        abundances,_ = posterior_function_returning_predictions((jtem,a))\n",
    "        model_abundances.append(abundances)\n",
    "        if j%100 == 0:\n",
    "            print(\"Calculating %s abundance set %d of %d\" %(name,j,length))\n",
    "            \n",
    "    # Save abundance table\n",
    "    np.save(\"Neural/\"+name+\"_abundances.npy\",model_abundances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,name in enumerate(names): # Create two identically distributed datasets\n",
    "    length = a.verif_test_sizes[i]\n",
    "    norm_grid = []\n",
    "    # Distribute data with prior width, but normalized with trial_widths as before\n",
    "    for _ in range(length):\n",
    "        norm_grid.append(np.random.normal(size = len(a.p0),\n",
    "                                          scale = np.array(a.test_widths)/np.array(a.training_widths)))\n",
    "     \n",
    "    # Convert data into normalized form\n",
    "    np.save(\"Neural/\"+name+\"_norm_grid.npy\",norm_grid)\n",
    "    \n",
    "    # Find the actual abundance grid\n",
    "    param_grid = [item*a.training_widths+a.p0 for item in norm_grid]\n",
    "    np.save(\"Neural/\"+name+\"_param_grid.npy\",param_grid)\n",
    "    \n",
    "    model_abundances = []\n",
    "    for j,jtem in enumerate(param_grid[:10]):\n",
    "        abundances,_ = posterior_function_returning_predictions((jtem,a))\n",
    "        model_abundances.append(abundances)\n",
    "        if j%2 == 0:\n",
    "            print(\"Calculating %s abundance set %d of %d\" %(name,j,length))\n",
    "            \n",
    "    # Save abundance table\n",
    "    np.save(\"Neural/\"+name+\"_abundances.npy\",model_abundances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import torch # Import PyTorch\n",
    "from torch.autograd import Variable\n",
    "from Chempy.parameter import ModelParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ModelParameters()\n",
    "\n",
    "n_train = a.training_size**len(a.p0) # Training data points\n",
    "n_neurons = a.neurons # No. neurons in layers\n",
    "\n",
    "# Load pre-processed training data\n",
    "tr_input = np.load('Neural/training_norm_grid.npy')\n",
    "tr_output = np.load('Neural/training_abundances.npy')\n",
    "\n",
    "# Calculate input dimension\n",
    "dim_in = tr_input.shape[1]\n",
    "dim_out = tr_output.shape[1]\n",
    "\n",
    "# Convert to torch variables\n",
    "tr_input = Variable(torch.from_numpy(tr_input)).type(torch.FloatTensor)\n",
    "tr_output = Variable(torch.from_numpy(tr_output), requires_grad=False).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Calculate neural network - use one hidden layer and no. neurons specified in parameter.py.\n",
    "# CHANGE these hyperparameters if needed\n",
    "\n",
    "# Remove the below ##############\n",
    "model = []\n",
    "a.learning_rate = 0.001 \n",
    "##############################\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(dim_in, a.neurons),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(a.neurons,dim_out)\n",
    ")\n",
    "loss_fn = torch.nn.L1Loss(size_average=True)\n",
    "\n",
    "# Use Adam optimizer with specified learning rate in parameter.py\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = a.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convergence counter\n",
    "current_loss = 1000 # High inital loss set\n",
    "count = 0\n",
    "t = 0\n",
    "\n",
    "# Used for plotting loss\n",
    "losslog = []\n",
    "epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural netowrk\n",
    "for i in range(a.epochs):\n",
    "    pred_output = model(tr_input)\n",
    "    loss = loss_fn(pred_output,tr_output)\n",
    "    optimizer.zero_grad() # Zero gradients initially\n",
    "    loss.backward() # Backpropagate\n",
    "    optimizer.step() # Update via Adam method\n",
    "    \n",
    "   \n",
    "    # Print cost\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        #print(loss.data[0])\n",
    "        losslog.append(loss.data[0])\n",
    "        epoch.append(i)\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch, losslog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert back to numpy arrays\n",
    "model_numpy = []\n",
    "for param in model.parameters():\n",
    "    model_numpy.append(param.data.numpy())\n",
    "    \n",
    "w_array_0 = model_numpy[0]\n",
    "b_array_0 = model_numpy[1]\n",
    "w_array_1 = model_numpy[2]\n",
    "b_array_1 = model_numpy[3]\n",
    "\n",
    "# Save parameters\n",
    "np.savez(\"Neural/neural_model.npz\",\n",
    "        w_array_0 = w_array_0,\n",
    "        w_array_1 = w_array_1,\n",
    "        b_array_0 = b_array_0,\n",
    "        b_array_1 = b_array_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_output(test_input):\n",
    "    \"\"\" This function will calculate the neural network predicted output.\n",
    "    The neural network must be trained first.\n",
    "    \n",
    "    Inputs: \n",
    "    test_input - Array containing unnormalized parameter values\n",
    "    neural_coeffs - Neural network weights, stored in neural_model.npz file\n",
    "    \n",
    "    Output: Neural network abundance prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in most recent neural coefficients\n",
    "    neural_coeffs = np.load('Neural/neural_model.npz')\n",
    "    w_array_0 = neural_coeffs['w_array_0']\n",
    "    w_array_1 = neural_coeffs['w_array_1']\n",
    "    b_array_0 = neural_coeffs['b_array_0']\n",
    "    b_array_1 = neural_coeffs['b_array_1']\n",
    "    \n",
    "    # Normalize data for neural network input\n",
    "    norm_data = (test_input - a.p0)/np.array(a.training_widths)\n",
    "    \n",
    "    # Calculate neural network output\n",
    "    hidden = np.maximum(0,np.dot(w_array_0, norm_data) + b_array_0)\n",
    "    output = np.dot(w_array_1, hidden) + b_array_1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Chempy.cem_function import posterior_function_returning_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average error using 1000 epochs for Verification dataset\n",
    "verif_param = np.load('Neural/verif_param_grid.npy')\n",
    "verif_abundances = np.load('Neural/verif_abundances.npy')\n",
    "\n",
    "for i in range(a.verif_test_sizes[0]):\n",
    "    predicted_abundances = neural_output(verif_param[i])\n",
    "    # Compute percentage error for each element\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the outputs\n",
    "\n",
    "%pylab inline \n",
    "# For testing\n",
    "\n",
    "x = np.arange(len(model_out))\n",
    "plt.plot(x,model_out,'r',label = 'Chempy')\n",
    "plt.plot(x,prediction_out,'b',label='Neural')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**\n",
    "- Vary optimizer, ReLU function, L1Loss to see which gives best results\n",
    "- Which regularization method are we using??\n",
    "- Check initialized weights - sqrt(2/n) thing\n",
    "- Plot loss function against epoch\n",
    "- Plot accuracy on verification dataset against epoch\n",
    "- Vary learning_rate\n",
    "- Plot accuracy on test data as 2D coloured plot in parameter space for each pair\n",
    "\n",
    "**MUST change the predictions to output in NON normalized space**\n",
    "\n",
    "**NB: if change ReLU function, must change in update above AND output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corner plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load verification dataset\n",
    "verif_param = np.load('Neural/verif_param_grid.npy')\n",
    "verif_abundances = np.load('Neural/verif_abundances.npy')\n",
    "\n",
    "# Calculate average percentage error in sample per element\n",
    "from Chempy.neural import neural_output\n",
    "error = []\n",
    "\n",
    "for i in range(len(verif_param)):\n",
    "    predicted_abundances = neural_output(verif_param[i])\n",
    "    # Compute percentage error for each element\n",
    "    error.append(np.absolute(predicted_abundances-verif_abundances[i]))\n",
    "\n",
    "# This contains absolute median error for each parameter set\n",
    "param_error = np.median(error,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corner plot infrastructure\n",
    "a=ModelParameters()\n",
    "\n",
    "# Load verification dataset\n",
    "data_v = np.array(np.load('Neural/verif_param_grid.npy'))\n",
    "data_tr = np.array(np.load('Neural/training_param_grid.npy'))\n",
    "\n",
    "\n",
    "parameter_names = [r'$\\alpha_\\mathrm{IMF}$',r'$\\log_{10}(\\mathrm{N_{Ia}})$',\n",
    "                   r'$\\log_{10}(\\tau_\\mathrm{Ia})$',r'$\\log_{10}(\\mathrm{SFE})$',\n",
    "                   r'$\\log_{10}(\\mathrm{SFR_{peak}})$',r'x_{out}']\n",
    "\n",
    "# Set up plot\n",
    "plt.clf()\n",
    "text_size = 12\n",
    "plt.rc('font', family='serif',size = text_size)\n",
    "plt.rc('xtick', labelsize=text_size)\n",
    "plt.rc('ytick', labelsize=text_size)\n",
    "plt.rc('axes', labelsize=text_size, lw=1.0)\n",
    "plt.rc('lines', linewidth = 1)\n",
    "plt.rcParams['ytick.major.pad']='8'\n",
    "plt.rcParams['text.latex.preamble']=[r\"\\usepackage{libertine}\"]\n",
    "params = {'text.usetex' : True,\n",
    "          'font.family' : 'libertine',\n",
    "          'text.latex.unicode': True,\n",
    "          }\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "fig,axes = plt.subplots(nrows = len(a.p0), ncols = len(a.p0),figsize=(14.69,8.0),dpi=300)\n",
    "alpha = 0.5\n",
    "lw=2 # Linewidth\n",
    "left = 0.1 # Left side of subplots\n",
    "right = 0.8 # Right side\n",
    "bottom = 0.075\n",
    "top = 0.97\n",
    "wspace = 0.0 # blankspace width between subplots\n",
    "hspace = 0.0 # blankspace height between subplots\n",
    "color_max = 0.15\n",
    "plt.subplots_adjust(left=left,bottom=bottom,right=right,top=top,wspace=wspace,hspace=hspace)\n",
    "\n",
    "\n",
    "for i in range(len(a.p0)):\n",
    "    for j in range(len(a.p0)):\n",
    "        axes[i,j].locator_params(nbins=4)\n",
    "        if j==1:\n",
    "            axes[i,j].locator_params(nbins=4)\n",
    "        if i==j:\n",
    "            counts,edges = np.histogram(np.asarray(data_v[:,j]),bins=10)\n",
    "            max_count = float(np.max(counts))\n",
    "            counts = np.divide(counts,max_count)\n",
    "            median = np.zeros(len(edges)-1)\n",
    "            for k in range(len(edges)-1):\n",
    "                choice = np.logical_and(np.greater(data_v[:,j],edges[k]),np.less(data_v[:,j],edges[k+1]))\n",
    "                error=np.extract(choice,param_error)\n",
    "                if len(error) != 0:\n",
    "                    median[k] = np.median(error)\n",
    "            colors = cm.plasma(median/color_max)\n",
    "            axes[i,j].bar(left = edges[:-1], height=counts, width = edges[1]-edges[0],\n",
    "                          color=colors,alpha=alpha, linewidth=0)\n",
    "            axes[i,j].set_xlim(min(data_v[:,j]),max(data_v[:,j]))\n",
    "            axes[i,j].set_ylim(0,1.05)\n",
    "            if j !=0:\n",
    "                plt.setp(axes[i,j].get_yticklabels(), visible=False)\n",
    "            axes[i,j].vlines(np.percentile(data_v[:,j],15.865),axes[i,j].get_ylim()[0],axes[i,j].get_ylim()[1], color = 'k',alpha=alpha,linewidth = lw,linestyle = 'dashed')    \n",
    "            axes[i,j].vlines(np.percentile(data_v[:,j],100-15.865),axes[i,j].get_ylim()[0],axes[i,j].get_ylim()[1], color = 'k',alpha=alpha,linewidth = lw,linestyle = 'dashed')    \n",
    "            axes[i,j].vlines(np.percentile(data_v[:,j],50),axes[i,j].get_ylim()[0],axes[i,j].get_ylim()[1], color = 'k',alpha=alpha,linewidth = lw)\n",
    "        if i>j:\n",
    "            if j !=0:\n",
    "                plt.setp(axes[i,j].get_yticklabels(), visible=False)\n",
    "            P1 = axes[i,j].scatter(data_v[:,i],data_v[:,j],marker='.',alpha=0.3,\n",
    "                                   c=param_error,vmin=0,vmax=color_max,cmap='plasma',s=1)\n",
    "            P2 = axes[i,j].scatter(data_tr[:,i],data_tr[:,j],c='k',marker='+',s=80)\n",
    "            axes[i,j].set_xlim(min(data_tr[:,i])-0.1,max(data_tr[:,i])+0.1)\n",
    "            axes[i,j].set_ylim(min(data_tr[:,j])-0.1,max(data_tr[:,j])+0.1)\n",
    "        if j>i:\n",
    "            axes[i,j].axis('off')\n",
    "        if i == len(a.p0)-1:\n",
    "            axes[i,j].set_xlabel(parameter_names[j])\n",
    "        if j ==0:\n",
    "            axes[i,j].set_ylabel(parameter_names[i])\n",
    "        if i==2 and j == 1:\n",
    "            cplot = axes[i,j].scatter(data_v[:,i],data_v[:,j],marker='.',alpha=0.3,\n",
    "                                      c=param_error,vmin=0,vmax=color_max,cmap='plasma',s=1)\n",
    "cax=fig.add_axes([0.82,0.06,0.02,0.9])\n",
    "plt.colorbar(cplot,cax=cax)\n",
    "plt.show()\n",
    "fig.savefig('Neural/corner_parameter_plot.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.rc('axes', labelsize=16, lw=0.3)\n",
    "plt.rc('text',usetex=True)\n",
    "\n",
    "# Create plot\n",
    "\n",
    "data_tr = np.load('Neural/training_param_grid.npy')\n",
    "data_v = np.load('Neural/verif_param_grid.npy')\n",
    "error = param_error\n",
    "\n",
    "# Use only 2 axes now\n",
    "x_tr = [item[0] for item in data_tr]\n",
    "y_tr = [item[3] for item in data_tr]\n",
    "x_v = [item[0] for item in data_v]\n",
    "y_v = [item[3] for item in data_v]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "P1 = ax.scatter(x_tr,y_tr,c='k',marker='+',s=120)\n",
    "ax = plt.gca()\n",
    "# Store axis limits\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "P2 = ax.scatter(x_v,y_v,marker='o',alpha=0.3,\n",
    "                c=error,s=1,cmap='plasma')\n",
    "# Replot this on top\n",
    "P1 = ax.scatter(x_tr,y_tr,c='k',marker='+',alpha=1,s=120)\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "fig.colorbar(P2,orientation='horizontal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spare Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "plt.clf()\n",
    "plt.rc('axes', labelsize=16, lw=0.3)\n",
    "plt.rc('text',usetex=True)\n",
    "\n",
    "#Create plot\n",
    "\n",
    "data_tr = np.load('Neural/training_param_grid.npy')\n",
    "data_v = np.load('Neural/verif_param_grid.npy')\n",
    "error = param_error\n",
    "\n",
    "#Use only 2 axes now\n",
    "\n",
    "x_tr = [item[0] for item in data_tr]\n",
    "y_tr = [item[3] for item in data_tr]\n",
    "x_v = [item[0] for item in data_v]\n",
    "y_v = [item[3] for item in data_v]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "P1 = ax.scatter(x_tr,y_tr,c='k',marker='+',s=120)\n",
    "ax = plt.gca()\n",
    "#Store axis limits\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "P2 = ax.scatter(x_v,y_v,marker='o',alpha=0.3,\n",
    "                c=error,s=2,cmap='plasma')\n",
    "#Replot this on top\n",
    "\n",
    "P1 = ax.scatter(x_tr,y_tr,c='k',marker='+',alpha=1,s=120)\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "fig.colorbar(P2,orientation='horizontal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from Chempy.parameter import ModelParameters\n",
    "a = ModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chempy.cem_function import cem_real2\n",
    "ab2,na2=cem_real2(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chempy.cem_function import posterior_function_predictions\n",
    "p,ab,na=posterior_function_predictions(a.p0,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chempy.neural import neural_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chempy.neural import neural_output\n",
    "names = a.elements_to_trace\n",
    "neural_abundances = neural_output(a.p0)\n",
    "names.append('Zcorona')\n",
    "names.append('SNratio')\n",
    "abundances=np.zeros((len(names),1))\n",
    "j=0\n",
    "for i,name in enumerate(names):\n",
    "    if name in a.neural_names:\n",
    "        abundances[i] =  neural_abundances[j]\n",
    "        j = j+1\n",
    "# Hack to fix bug\n",
    "del a\n",
    "a=ModelParameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46697587],\n",
       "       [ 0.10105052],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [-0.19060413],\n",
       "       [ 0.01870418],\n",
       "       [ 0.        ],\n",
       "       [-0.11551477],\n",
       "       [ 0.1340243 ],\n",
       "       [ 0.        ],\n",
       "       [-0.07493073],\n",
       "       [ 0.23446705],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [-0.1770244 ],\n",
       "       [-0.51326051],\n",
       "       [ 0.        ],\n",
       "       [ 0.19791537],\n",
       "       [ 0.15522758],\n",
       "       [ 0.12359748],\n",
       "       [ 0.69310467],\n",
       "       [ 0.45968022],\n",
       "       [ 0.16384264],\n",
       "       [ 0.3627465 ],\n",
       "       [ 0.40606143],\n",
       "       [ 0.24930715],\n",
       "       [ 0.        ],\n",
       "       [ 0.3018295 ],\n",
       "       [-0.27624317],\n",
       "       [-0.16511348],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abundances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abundances=np.zeros((len(names),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "for i,name in enumerate(names):\n",
    "    if name in a.neural_names:\n",
    "        abundances[i] =  neural_abundances[j]\n",
    "        print(name)\n",
    "        j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sol_dat=np.load('Chempy/input/stars/Proto-sun.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_dat.dtype.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_list = []\n",
    "for item in a.elements_to_trace:\n",
    "    if item in list(sol_dat.dtype.names):\n",
    "        neural_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
