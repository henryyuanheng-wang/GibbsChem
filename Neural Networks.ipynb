{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the code to produce a sample set for and train a neural network.**\n",
    "All python source code is in the neural.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from Chempy.parameter import ModelParameters\n",
    "a = ModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First create the training dataset\n",
    "from Chempy.neural import training_data\n",
    "# %timeit -r 1 -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was run on a faster machine, taking 2 hours, 26 minutes and 28 seconds to calculate a training dataset of length 15625. This is stored in the 'Neural/' folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now create the datasets for model verification (i.e. hyperparameter constraints)\n",
    "# and for final testing\n",
    "\n",
    "from Chempy.neural import verification_and_testing\n",
    "# %timeit -r 1 -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was again run on a faster PC. The runtime was 19 minutes and 10 seconds, producing the verif_param_grid, verif_abundances, test_param_grid, test_abundances npy data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "## The code below will allow optimization of hyperparameter\n",
    "%pylab inline\n",
    "from Chempy.parameter import ModelParameters\n",
    "a = ModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network 1 of 7\n",
      "Training epoch 0 of 5000 complete\n",
      "Training epoch 1000 of 5000 complete\n"
     ]
    }
   ],
   "source": [
    "# To plot neural network error against learning rate for optimization\n",
    "\n",
    "from Chempy.neural import create_network,neural_errors\n",
    "\n",
    "#learning_rate = [0.001,0.003,0.005,0.007,0.009] #\n",
    "learning_rate = 10**np.linspace(-3.5,-2,7)\n",
    "upper = np.zeros((len(learning_rate),1))\n",
    "lower = np.zeros((len(learning_rate),1))\n",
    "median = np.zeros((len(learning_rate),1))\n",
    "\n",
    "for i,rate in enumerate(learning_rate):\n",
    "    print(\"Creating network %d of %d\" %(i+1,len(learning_rate)))\n",
    "    create_network(learning_rate=rate,Plot=False)\n",
    "    param_error = neural_errors('verif')\n",
    "    median[i] = np.median(param_error)\n",
    "    upper[i] = np.percentile(param_error,15.865)\n",
    "    lower[i] = np.percentile(param_error,100-15.865)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.errorbar((learning_rate),median,yerr=[median-lower,upper-median])\n",
    "plt.ylabel('Median L1 error in network across all elements (dex)')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.title('Optimization of Learning Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Optimal Set\n",
    "*Now using test dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create and train the neural network for optimal hyperparameters\n",
    "%pylab inline\n",
    "from Chempy.neural import create_network\n",
    "learning_rate = 0.003\n",
    "epoch, loss = create_network(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the corner plot\n",
    "from Chempy.neural import neural_corner_plot, neural_errors\n",
    "param_error = neural_errors('test')\n",
    "print('Mean error is %.5f, with standard deviation %.5f' %(np.median(param_error),np.std(param_error)))\n",
    "neural_corner_plot('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=plt.hist(param_error,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chempy.neural import neural_errors,neural_output\n",
    "param_error = neural_errors('test')\n",
    "param_grid = np.load('Neural/test_param_grid.npy')\n",
    "chempy_data = np.load('Neural/test_abundances.npy')\n",
    "neural_data = []\n",
    "for i in range(len(param_grid)):\n",
    "    neural_data.append(neural_output(param_grid[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ABUNDANCE COMPARISON\n",
    "\n",
    "# Using first random datapoint in testset\n",
    "\n",
    "# Prepare plot\n",
    "fig = plt.figure(figsize=(30.69,8.27), dpi=100)\n",
    "plt.clf()\n",
    "text_size = 15\n",
    "plt.rc('font', family='serif',size = text_size)\n",
    "plt.rc('xtick', labelsize=text_size)\n",
    "plt.rc('ytick', labelsize=text_size)\n",
    "plt.rc('axes', labelsize=text_size, lw=1.)\n",
    "plt.rc('lines', linewidth = 1.)\n",
    "plt.rcParams['ytick.major.pad']='8'\n",
    "plt.rcParams['text.latex.preamble']=[r\"\\usepackage{libertine}\"]\n",
    "params = {'text.usetex' : True,'font.size' : 16,'font.family' : 'libertine','text.latex.unicode': True}\n",
    "ax = fig.add_subplot(111)\n",
    "abundance_names = []\n",
    "proto_sun = np.load('Chempy/input/stars/Proto-sun.npy')\n",
    "for item in proto_sun.dtype.names[:-1]:\n",
    "    if item != 'Fe':\n",
    "        abundance_names.append('[%s/Fe]' %(item))\n",
    "    else:\n",
    "        abundance_names.append('[Fe/H]')\n",
    "plt.xticks(np.arange(len(a.element_names)),abundance_names)\n",
    "\n",
    "plt.plot(neural_data[0],'r',label='Neural')\n",
    "plt.plot(chempy_data[0],'g',label='Chempy')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Abundances relative to solar in dex\")\n",
    "plt.xlabel(\"Element\")\n",
    "plt.title(\"Abundance plot\")\n",
    "savefig('Neural/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES / TO DO (see other file also)**\n",
    "- *Change the diagonal to be correct histogram of data - DONE*\n",
    "- *Color histogram by mean error in that bar - DONE*\n",
    "- *Add other params - DONE*\n",
    "- *Sort axis names - DONE*\n",
    "- SORT axis sizes (see plot_mcmc.py)\n",
    "- Vectorize neural output calculation\n",
    "- Add support for corner plot to take either dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
