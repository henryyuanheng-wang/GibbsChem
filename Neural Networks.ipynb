{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the code to produce a sample set for and train a neural network.**\n",
    "All python source code is in the neural.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from Chempy.parameter import ModelParameters\n",
    "a = ModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the training dataset\n",
    "from Chempy.neural import training_data\n",
    "# %timeit -r 1 -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above was run on a faster machine, taking 2 hours, 26 minutes and 28 seconds to calculate a training dataset of length 15625. This is stored in the 'Neural/' folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the datasets for model verification (i.e. hyperparameter constraints)\n",
    "# and for final testing\n",
    "\n",
    "from Chempy.neural import verification_and_testing\n",
    "# %timeit -r 1 -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was again run on a faster PC. The runtime was 19 minutes and 10 seconds, producing the verif_param_grid, verif_abundances, test_param_grid, test_abundances npy data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 of 1000 complete\n",
      "Training epoch 100 of 1000 complete\n",
      "Training epoch 200 of 1000 complete\n",
      "Training epoch 300 of 1000 complete\n",
      "Training epoch 400 of 1000 complete\n",
      "Training epoch 500 of 1000 complete\n",
      "Training epoch 600 of 1000 complete\n",
      "Training epoch 700 of 1000 complete\n"
     ]
    }
   ],
   "source": [
    "# Now create and train the neural network\n",
    "from Chempy.neural import create_network\n",
    "epoch, loss = create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the corner plot\n",
    "from Chempy.neural import neural_corner_plot, neural_errors\n",
    "param_error = neural_errors('verif')\n",
    "print(np.median(param_error))\n",
    "print(np.std(param_error))\n",
    "neural_corner_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load verification dataset\n",
    "verif_param = np.load('Neural/verif_param_grid.npy')\n",
    "verif_abundances = np.load('Neural/verif_abundances.npy')\n",
    "\n",
    "# Calculate average percentage error in sample per element\n",
    "from Chempy.neural import neural_output\n",
    "error = []\n",
    "\n",
    "for i in range(a.verif_test_sizes[0]):\n",
    "    predicted_abundances = neural_output(verif_param[i])\n",
    "    # Compute percentage error for each element\n",
    "    error.append(np.absolute(predicted_abundances-verif_abundances[i]))\n",
    "element_error = np.mean(error,axis=0)\n",
    "print(np.max(element_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif_param = np.load('Neural/verif_param_grid.npy')\n",
    "verif_abundances = np.load('Neural/verif_abundances.npy')\n",
    "verif_abundances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare plot\n",
    "fig = plt.figure(figsize=(30.69,8.27), dpi=100)\n",
    "plt.clf()\n",
    "text_size = 15\n",
    "plt.rc('font', family='serif',size = text_size)\n",
    "plt.rc('xtick', labelsize=text_size)\n",
    "plt.rc('ytick', labelsize=text_size)\n",
    "plt.rc('axes', labelsize=text_size, lw=1.)\n",
    "plt.rc('lines', linewidth = 1.)\n",
    "plt.rcParams['ytick.major.pad']='8'\n",
    "plt.rcParams['text.latex.preamble']=[r\"\\usepackage{libertine}\"]\n",
    "params = {'text.usetex' : True,'font.size' : 16,'font.family' : 'libertine','text.latex.unicode': True}\n",
    "ax = fig.add_subplot(111)\n",
    "abundance_names = []\n",
    "proto_sun = np.load('Chempy/input/stars/Proto-sun.npy')\n",
    "for item in proto_sun.dtype.names[:-1]:\n",
    "    if item != 'Fe':\n",
    "        abundance_names.append('[%s/Fe]' %(item))\n",
    "    else:\n",
    "        abundance_names.append('[Fe/H]')\n",
    "plt.xticks(np.arange(len(a.element_names)),abundance_names)\n",
    "\n",
    "plt.plot(predicted_abundances,'r',label='Neural')\n",
    "plt.plot(verif_abundances[1],'g',label='Chempy')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Abundances relative to solar in dex\")\n",
    "plt.xlabel(\"Element\")\n",
    "plt.title(\"Abundance plot\")\n",
    "savefig('Neural/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create corner plot for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load verification dataset\n",
    "verif_param = np.load('Neural/verif_param_grid.npy')\n",
    "verif_abundances = np.load('Neural/verif_abundances.npy')\n",
    "\n",
    "# Calculate average percentage error in sample per element\n",
    "from Chempy.neural import neural_output\n",
    "error = []\n",
    "\n",
    "for i in range(len(verif_param)):\n",
    "    predicted_abundances = neural_output(verif_param[i])\n",
    "    # Compute percentage error for each element\n",
    "    error.append(np.absolute(predicted_abundances-verif_abundances[i]))\n",
    "\n",
    "# This contains absolute median error for each parameter set\n",
    "param_error = np.median(error,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.verif_test_sizes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.rc('axes', labelsize=16, lw=0.3)\n",
    "plt.rc('text',usetex=True)\n",
    "\n",
    "# Create plot\n",
    "\n",
    "data_tr = np.load('Neural/training_param_grid.npy')\n",
    "data_v = np.load('Neural/verif_param_grid.npy')\n",
    "error = param_error\n",
    "\n",
    "# Use only 2 axes now\n",
    "x_tr = [item[0] for item in data_tr]\n",
    "y_tr = [item[3] for item in data_tr]\n",
    "x_v = [item[0] for item in data_v]\n",
    "y_v = [item[3] for item in data_v]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "P1 = ax.scatter(x_tr,y_tr,c='k',marker='+',s=120)\n",
    "ax = plt.gca()\n",
    "# Store axis limits\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "P2 = ax.scatter(x_v,y_v,marker='o',alpha=0.3,\n",
    "                c=error,s=1,cmap='plasma')\n",
    "# Replot this on top\n",
    "P1 = ax.scatter(x_tr,y_tr,c='k',marker='+',alpha=1,s=120)\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "fig.colorbar(P2,orientation='horizontal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES / TO DO (see other file also)**\n",
    "- *Change the diagonal to be correct histogram of data - DONE*\n",
    "- *Color histogram by mean error in that bar - DONE*\n",
    "- *Add other params - DONE*\n",
    "- *Sort axis names - DONE*\n",
    "- SORT axis sizes (see plot_mcmc.py)\n",
    "- Vectorize neural output calculation\n",
    "- Add support for corner plot to take either dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
