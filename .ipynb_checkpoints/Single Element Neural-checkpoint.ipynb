{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate neural predictions for a single element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from Chempy.parameter import ModelParameters\n",
    "a = ModelParameters\n",
    "a.neurons = 10 # 10 neurons\n",
    "a.epochs = 1000\n",
    "learning_rate = 0.01\n",
    "element_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_neural(lr,element_index,neurons=a.neurons):\n",
    "\n",
    "    training_abundances = np.load('Neural/training_abundances.npy')\n",
    "    tr_input = np.load('Neural/training_norm_grid.npy')\n",
    "    tr_output = training_abundances[:,element_index]\n",
    "\n",
    "    dim_in = tr_input.shape[1]\n",
    "    dim_out = 1\n",
    "\n",
    "    tr_input = Variable(torch.from_numpy(tr_input)).type(torch.FloatTensor)\n",
    "    tr_output = Variable(torch.from_numpy(tr_output), requires_grad=False).type(torch.FloatTensor)\n",
    "\n",
    "    model = []\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(dim_in,neurons),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(neurons,dim_out)\n",
    "        )\n",
    "    loss_fn = torch.nn.L1Loss(size_average=True)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    losslog = []\n",
    "    epoch = []\n",
    "\n",
    "    # Train neural network\n",
    "    for i in range(a.epochs):\n",
    "        pred_output = model(tr_input)\n",
    "        loss = loss_fn(pred_output, tr_output)\n",
    "        optimizer.zero_grad() # Initially zero gradient\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update via optimizer\n",
    "\n",
    "        # Output loss\n",
    "        if i % 3 ==0:\n",
    "            losslog.append(loss.data[0])\n",
    "            epoch.append(i)\n",
    "        if i % 100==0:\n",
    "            print(\"Training epoch %d of %d complete\" %(i,a.epochs))\n",
    "\n",
    "    # Convert weights to numpy arrays\n",
    "    model_numpy = []\n",
    "    for param in model.parameters():\n",
    "        model_numpy.append(param.data.numpy())\n",
    "        \n",
    "    w_array_0=model_numpy[0]\n",
    "    b_array_0=model_numpy[1]\n",
    "    w_array_1=model_numpy[2]\n",
    "    b_array_1=model_numpy[3]\n",
    "\n",
    "    # Test network\n",
    "    all_abun = np.load('Neural/verif_abundances.npy')\n",
    "    test_abun = all_abun[:,element_index]\n",
    "    test_param = np.load('Neural/verif_param_grid.npy')\n",
    "\n",
    "    err = []\n",
    "    for i,item in enumerate(test_param):\n",
    "        norm_data = (item - a.p0)/np.array(a.training_widths)\n",
    "        hidden1 = np.tanh(np.array(np.dot(w_array_0,norm_data)+b_array_0))\n",
    "        output = np.dot(w_array_1, hidden1)+b_array_1\n",
    "        err.append(np.absolute(output-test_abun[i]))\n",
    "    print(np.median(err),max(err),min(err))\n",
    "    return np.median(err),np.std(err),epoch,losslog,w_array_0,b_array_0,w_array_1,b_array_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error = []\n",
    "stderr = []\n",
    "learning_rate = [0.01]\n",
    "neurons = 10\n",
    "for lr in learning_rate:\n",
    "    print(lr)\n",
    "    er,ste,epoch,losslog,_,_,_,_ = single_neural(lr,element_index,neurons)\n",
    "    error.append(er)\n",
    "    stderr.append(ste)\n",
    "    plt.plot(epoch,losslog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.errorbar((learning_rate),error,yerr=stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 of 1000 complete\n",
      "Training epoch 100 of 1000 complete\n",
      "Training epoch 200 of 1000 complete\n",
      "Training epoch 300 of 1000 complete\n",
      "Training epoch 400 of 1000 complete\n",
      "Training epoch 500 of 1000 complete\n",
      "Training epoch 600 of 1000 complete\n",
      "Training epoch 700 of 1000 complete\n",
      "Training epoch 800 of 1000 complete\n",
      "Training epoch 900 of 1000 complete\n",
      "0.0199530954181 [ 0.57194214] [  1.00853672e-05]\n"
     ]
    }
   ],
   "source": [
    "_,_,_,_,w_array_0,b_array_0,w_array_1,b_array_1 = single_neural(learning_rate,element_index,a.neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w_array_0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-84c30cf6ca11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mrescaled_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_widths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mhidden1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_array_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrescaled_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb_array_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mneural_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_array_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb_array_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mabun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mposterior_function_returning_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w_array_0' is not defined"
     ]
    }
   ],
   "source": [
    "# Create random parameters up to 3 sigma from mean\n",
    "from Chempy.cem_function import posterior_function_returning_predictions\n",
    "err = []\n",
    "sigma = np.array([0.3,0.3,0.3,0.3,0.1,0.1])\n",
    "Ntest = 100\n",
    "param = np.zeros((len(Ntest),len(a.p0)))\n",
    "for i in range(Ntest):\n",
    "    print(i)\n",
    "    param[i,:] = a.p0+3*sigma*np.random.rand(6)\n",
    "    rescaled_param = (param[i,:]-a.p0)/np.array(a.training_widths)\n",
    "    hidden1 = np.tanh(np.array(np.dot(w_array_0,rescaled_param)+b_array_0))\n",
    "    neural_output = np.dot(w_array_1, hidden1)+b_array_1\n",
    "    abun,_= posterior_function_returning_predictions((param[i,:],a))\n",
    "    chem_output = abun(element_index)\n",
    "    err.append(np.absolute(chem_output-neural_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif= np.load('Neural/verif_param_grid.npy')\n",
    "x1 = verif[:,0]\n",
    "x2 = verif[:,1]\n",
    "plt.scatter(x1,x2,s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_train = a.training_size**len(a.p0) # No. data points in training set\n",
    "\n",
    "\t# Load pre-processed training data\n",
    "\ttr_input = np.load('Neural/training_norm_grid.npy')\n",
    "\ttr_output = np.load('Neural/training_abundances.npy')\n",
    "\n",
    "\t# Calculate the model dimensions\n",
    "\tdim_in = tr_input.shape[1]\n",
    "\tdim_out = tr_output.shape[1]\n",
    "\n",
    "\t# Convert to torch variables\n",
    "\ttr_input = Variable(torch.from_numpy(tr_input)).type(torch.FloatTensor)\n",
    "\ttr_output = Variable(torch.from_numpy(tr_output), requires_grad=False).type(torch.FloatTensor)\n",
    "\n",
    "\t# Set up the neural network, with one hidden layer\n",
    "\tmodel = [] # Remove any previous network\n",
    "\n",
    "\tmodel = torch.nn.Sequential(\n",
    "\t\t\t\ttorch.nn.Linear(dim_in,a.neurons),\n",
    "\t\t\t\ttorch.nn.Tanh(),\n",
    "\t\t\t\ttorch.nn.Linear(a.neurons,dim_out)\n",
    "\t\t\t\t)\n",
    "\tloss_fn = torch.nn.L1Loss(size_average=True)\n",
    "\n",
    "\t# Use Adam optimizer with learning rate specified in parameter.py\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\t#scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "\t# For loss records\n",
    "\tlosslog = []\n",
    "\tepoch = []\n",
    "\n",
    "\t# Train neural network\n",
    "\tfor i in range(a.epochs):\n",
    "\t\tpred_output = model(tr_input)\n",
    "\t\tloss = loss_fn(pred_output, tr_output)\n",
    "\t\toptimizer.zero_grad() # Initially zero gradient\n",
    "\t\tloss.backward() # Backpropagation\n",
    "\t\toptimizer.step() # Update via optimizer\n",
    "\t\t#scheduler.step(loss)\n",
    "\n",
    "\t\t# Output loss\n",
    "\t\tif i % 3 ==0:\n",
    "\t\t\tlosslog.append(loss.data[0])\n",
    "\t\t\tepoch.append(i)\n",
    "\t\tif i % 1000==0:\n",
    "\t\t\tprint(\"Training epoch %d of %d complete\" %(i,a.epochs))\n",
    "\n",
    "\t# Convert weights to numpy arrays\n",
    "\tmodel_numpy = []\n",
    "\tfor param in model.parameters():\n",
    "\t\tmodel_numpy.append(param.data.numpy())\n",
    "\n",
    "\tnp.savez(\"Neural/neural_model.npz\",\n",
    "\t\t\t\tw_array_0=model_numpy[0],\n",
    "\t\t\t\tb_array_0=model_numpy[1],\n",
    "\t\t\t\tw_array_1=model_numpy[2],\n",
    "\t\t\t\tb_array_1=model_numpy[3])\n",
    "\n",
    "\tif Plot==True:\n",
    "\t\tplt.plot(epoch,losslog,label=learning_rate)\n",
    "\t\tplt.ylabel(\"L1 Loss value\")\n",
    "\t\tplt.xlabel(\"Epoch\")\n",
    "\t\tplt.title(\"Loss plot\")\n",
    "\t\tplt.legend()\n",
    "\t\tplt.show()\n",
    "\t\tplt.savefig('Neural/lossplot')\n",
    "\n",
    "\treturn epoch, losslog\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
