{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training data for use with the neural network. We must also save the affine scaling parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Chempy.parameter import ModelParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ModelParameters()\n",
    "\n",
    "# Define training size\n",
    "N = 8.  # 6 grid points per parameter\n",
    "#widths = a.training_widths # Gaussian training widths for parameters\n",
    "widths = [0.3, 0.3, 0.3, 0.1, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array for log time\n",
    "time_array = np.linspace(0.5,2.5,int(N)) # make array wider than observed time dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm as gaussian\n",
    "prob = np.linspace(1/(N+1), 1-1/(N+1), int(N))\n",
    "grids = [gaussian.ppf(prob) for _ in range(len(a.p0))] # Normalize to unit Gaussian\n",
    "grids.append(time_array) ## add time array to this grid\n",
    "norm_grid = np.array(np.meshgrid(*grids)).T.reshape(-1,len(a.p0)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid in parameter space\n",
    "full_widths = list(widths)+[1.]\n",
    "means = list(a.p0)+[0.]\n",
    "param_grid = [np.asarray(item)*full_widths+means for item in norm_grid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the two grids to file:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```norm_grid``` is an array of $N^6$ sets of 6d parameter vectors.\n",
    "- NB: The first five parameters are **normalized**: to get true values multiply by the width and add the mean value.\n",
    "- The time parameter is **not** normalized (since it is not drawn froma  Gaussian here)\n",
    "\n",
    "\n",
    "- ```Param_grid``` is of the same form but with the full grid of parameters (with no normalization applied)\n",
    "- Each element is of the form $(\\Theta_1,\\Theta_2,\\Lambda_1,\\Lambda_2,\\Lambda_3,T)$ \n",
    "\n",
    "\n",
    "- ```means``` gives the prior means for the parameters (for reconstruction from the normalized parameters)\n",
    "- ```full_widths``` gives the scaling widths to multiply the normalized parameters by to get the true parameters.\n",
    "\n",
    "NB: we include a 0 mean and 1. width for the time parameter so we can always write true_parameter = norm_parameter x width + means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('APOGEE Training Data',norm_grid=norm_grid,param_grid=param_grid,full_widths=full_widths,means=means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and play with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../Network Training Data/APOGEE Training Data.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f0092e10d07c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'pylab inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_x_dat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Network Training Data/APOGEE Training Data.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_x_dat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'norm_grid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtraining_y_dat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Network Training Data/APOGEE Training Predictions.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_y_dat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abundances'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oliverphilcox/anaconda2/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../Network Training Data/APOGEE Training Data.npz'"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "training_x_dat = np.load('../Network Training Data/APOGEE Training Data.npz')\n",
    "trainX=training_x_dat['norm_grid']\n",
    "training_y_dat = np.load('../Network Training Data/APOGEE Training Predictions.npz')\n",
    "trainY=training_y_dat['abundances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 10\n",
    "learning_rate=0.01\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named torch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9bb773ff192a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named torch"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "n_train = len(trainY)\n",
    "\n",
    "# Calculate the model dimensions\n",
    "dim_in = trainX.shape[1]\n",
    "dim_out = trainY.shape[1]\n",
    "\n",
    "# Convert to torch variables\n",
    "tr_input = Variable(torch.from_numpy(trainX)).type(torch.FloatTensor)\n",
    "tr_output = Variable(torch.from_numpy(trainY), requires_grad=False).type(torch.FloatTensor)\n",
    "\n",
    "# Set up the neural network, with one hidden layer\n",
    "model = [] # Remove any previous network\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(dim_in,neurons),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(neurons,dim_out)\n",
    "    )\n",
    "loss_fn = torch.nn.L1Loss(size_average=True)\n",
    "\n",
    "# Use Adam optimizer with learning rate specified in parameter.py\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# For loss records\n",
    "losslog = []\n",
    "epoch = []\n",
    "\n",
    "# Train neural network\n",
    "for i in range(epochs):\n",
    "    pred_output = model(tr_input)\n",
    "    loss = loss_fn(pred_output, tr_output)\n",
    "    optimizer.zero_grad() # Initially zero gradient\n",
    "    loss.backward() # Backpropagation\n",
    "    optimizer.step() # Update via optimizer\n",
    "        \n",
    "    if i % 3 ==0:\n",
    "        losslog.append(loss.data[0])\n",
    "        epoch.append(i)\n",
    "    if i % 100==0:\n",
    "        print(\"Training epoch %d of %d complete\" %(i,a.epochs))\n",
    "\n",
    "# Convert weights to numpy arrays\n",
    "model_numpy = []\n",
    "for param in model.parameters():\n",
    "    model_numpy.append(param.data.numpy())\n",
    "\n",
    "np.savez(\"Neural/neural_model.npz\",\n",
    "            w_array_0=model_numpy[0],\n",
    "            b_array_0=model_numpy[1],\n",
    "            w_array_1=model_numpy[2],\n",
    "            b_array_1=model_numpy[3])\n",
    "\n",
    "plt.plot(epoch,losslog,label=learning_rate)\n",
    "plt.ylabel(\"L1 Loss value\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Loss plot\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_network(learning_rate=a.learning_rate,Plot=True):\n",
    "\t\"\"\" Function to create and train the neural network - this overwrites any previous network.\n",
    "\tInputs:\n",
    "\t\tlearning_rate of model (default is in parameter.py)\n",
    "\t\tPlot - whether to plot loss curve against epoch\n",
    "\tOutputs:\n",
    "\t\tepochs - Training epoch number (outputted each 10 epochs)\n",
    "\t\tlosslog - loss value for each 10 epochs\n",
    "\t\tPlot of loss against epoch (if Plot=True)\n",
    "\n",
    "\t\tNeural/neural_model.npz - Saved .npz file with model weights\n",
    "\t\"\"\"\n",
    "\n",
    "\timport torch\n",
    "\tfrom torch.autograd import Variable\n",
    "\t#from torch.optim import lr_scheduler\n",
    "\n",
    "\t# Load parameters\n",
    "\tn_train = a.training_size**len(a.p0) # No. data points in training set\n",
    "\n",
    "\t# Load pre-processed training data\n",
    "\ttr_input = np.load('Neural/training_norm_grid.npy')\n",
    "\ttr_output = np.load('Neural/training_abundances.npy')\n",
    "\n",
    "\t# Calculate the model dimensions\n",
    "\tdim_in = tr_input.shape[1]\n",
    "\tdim_out = tr_output.shape[1]\n",
    "\n",
    "\t# Convert to torch variables\n",
    "\ttr_input = Variable(torch.from_numpy(tr_input)).type(torch.FloatTensor)\n",
    "\ttr_output = Variable(torch.from_numpy(tr_output), requires_grad=False).type(torch.FloatTensor)\n",
    "\n",
    "\t# Set up the neural network, with one hidden layer\n",
    "\tmodel = [] # Remove any previous network\n",
    "\n",
    "\tmodel = torch.nn.Sequential(\n",
    "\t\t\t\ttorch.nn.Linear(dim_in,a.neurons),\n",
    "\t\t\t\ttorch.nn.Tanh(),\n",
    "\t\t\t\ttorch.nn.Linear(a.neurons,dim_out)\n",
    "\t\t\t\t)\n",
    "\tloss_fn = torch.nn.L1Loss(size_average=True)\n",
    "\n",
    "\t# Use Adam optimizer with learning rate specified in parameter.py\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\t#scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "\t# For loss records\n",
    "\tlosslog = []\n",
    "\tepoch = []\n",
    "\n",
    "\t# Train neural network\n",
    "\tfor i in range(a.epochs):\n",
    "\t\tpred_output = model(tr_input)\n",
    "\t\tloss = loss_fn(pred_output, tr_output)\n",
    "\t\toptimizer.zero_grad() # Initially zero gradient\n",
    "\t\tloss.backward() # Backpropagation\n",
    "\t\toptimizer.step() # Update via optimizer\n",
    "\t\t#scheduler.step(loss)\n",
    "\n",
    "\t\t# Output loss\n",
    "\t\tif i % 3 ==0:\n",
    "\t\t\tlosslog.append(loss.data[0])\n",
    "\t\t\tepoch.append(i)\n",
    "\t\tif i % 100==0:\n",
    "\t\t\tprint(\"Training epoch %d of %d complete\" %(i,a.epochs))\n",
    "\n",
    "\t# Convert weights to numpy arrays\n",
    "\tmodel_numpy = []\n",
    "\tfor param in model.parameters():\n",
    "\t\tmodel_numpy.append(param.data.numpy())\n",
    "\n",
    "\tnp.savez(\"Neural/neural_model.npz\",\n",
    "\t\t\t\tw_array_0=model_numpy[0],\n",
    "\t\t\t\tb_array_0=model_numpy[1],\n",
    "\t\t\t\tw_array_1=model_numpy[2],\n",
    "\t\t\t\tb_array_1=model_numpy[3])\n",
    "\n",
    "\tif Plot==True:\n",
    "\t\tplt.plot(epoch,losslog,label=learning_rate)\n",
    "\t\tplt.ylabel(\"L1 Loss value\")\n",
    "\t\tplt.xlabel(\"Epoch\")\n",
    "\t\tplt.title(\"Loss plot\")\n",
    "\t\tplt.legend()\n",
    "\t\tplt.show()\n",
    "\t\tplt.savefig('Neural/lossplot')\n",
    "\n",
    "\treturn epoch, losslog\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
