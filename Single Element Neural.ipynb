{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate neural predictions for a single element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from Chempy.parameter import ModelParameters\n",
    "a = ModelParameters\n",
    "a.neurons = 10 # 10 neurons\n",
    "a.epochs = 1000\n",
    "learning_rate = 0.01\n",
    "element_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_neural(lr,element_index,neurons=a.neurons):\n",
    "\n",
    "    training_abundances = np.load('Neural/training_abundances.npy')\n",
    "    tr_input = np.load('Neural/training_norm_grid.npy')\n",
    "    tr_output = training_abundances[:,element_index]\n",
    "\n",
    "    dim_in = tr_input.shape[1]\n",
    "    dim_out = 1\n",
    "\n",
    "    tr_input = Variable(torch.from_numpy(tr_input)).type(torch.FloatTensor)\n",
    "    tr_output = Variable(torch.from_numpy(tr_output), requires_grad=False).type(torch.FloatTensor)\n",
    "\n",
    "    model = []\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(dim_in,neurons),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(neurons,dim_out)\n",
    "        )\n",
    "    loss_fn = torch.nn.L1Loss(size_average=True)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    losslog = []\n",
    "    epoch = []\n",
    "\n",
    "    # Train neural network\n",
    "    for i in range(a.epochs):\n",
    "        pred_output = model(tr_input)\n",
    "        loss = loss_fn(pred_output, tr_output)\n",
    "        optimizer.zero_grad() # Initially zero gradient\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update via optimizer\n",
    "\n",
    "        # Output loss\n",
    "        if i % 3 ==0:\n",
    "            losslog.append(loss.data[0])\n",
    "            epoch.append(i)\n",
    "        if i % 100==0:\n",
    "            print(\"Training epoch %d of %d complete\" %(i,a.epochs))\n",
    "\n",
    "    # Convert weights to numpy arrays\n",
    "    model_numpy = []\n",
    "    for param in model.parameters():\n",
    "        model_numpy.append(param.data.numpy())\n",
    "        \n",
    "    w_array_0=model_numpy[0]\n",
    "    b_array_0=model_numpy[1]\n",
    "    w_array_1=model_numpy[2]\n",
    "    b_array_1=model_numpy[3]\n",
    "    \n",
    "    np.save('SingleElement/w0.npy',w_array_0)\n",
    "    np.save('SingleElement/w1.npy',w_array_1)\n",
    "    np.save('SingleElement/b0.npy',b_array_0)\n",
    "    np.save('SingleElement/b1.npy',b_array_1)\n",
    "    \n",
    "    # Test network\n",
    "    all_abun = np.load('Neural/verif_abundances.npy')\n",
    "    test_abun = all_abun[:,element_index]\n",
    "    test_param = np.load('Neural/verif_param_grid.npy')\n",
    "\n",
    "    err = []\n",
    "    for i,item in enumerate(test_param):\n",
    "        norm_data = (item - a.p0)/np.array(a.training_widths)\n",
    "        hidden1 = np.tanh(np.array(np.dot(w_array_0,norm_data)+b_array_0))\n",
    "        output = np.dot(w_array_1, hidden1)+b_array_1\n",
    "        err.append(np.absolute(output-test_abun[i]))\n",
    "    print(np.median(err),max(err),min(err))\n",
    "    return np.median(err),np.std(err),epoch,losslog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_pred(test_param):\n",
    "    \"\"\"\n",
    "    Predict element output from pre-trained neural network\n",
    "    \"\"\"\n",
    "    # Load coefficients\n",
    "    w_array_0 = np.load('SingleElement/w0.npy')\n",
    "    w_array_1 = np.load('SingleElement/w1.npy')\n",
    "    b_array_0 = np.load('SingleElement/b0.npy')\n",
    "    b_array_1 = np.load('SingleElement/b1.npy')\n",
    "        \n",
    "    norm_data = (test_param - a.p0)/np.array(a.training_widths)\n",
    "    hidden1 = np.tanh(np.array(np.dot(w_array_0,norm_data)+b_array_0))\n",
    "    output = np.dot(w_array_1, hidden1)+b_array_1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error = []\n",
    "stderr = []\n",
    "learning_rate = [0.01]\n",
    "neurons = 10\n",
    "for lr in learning_rate:\n",
    "    print(lr)\n",
    "    er,ste,epoch,losslog,w_array_0,b_array_0,w_array_1,b_array_1 = single_neural(lr,element_index,neurons)\n",
    "    error.append(er)\n",
    "    stderr.append(ste)\n",
    "    plt.plot(epoch,losslog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.errorbar((learning_rate),error,yerr=stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create random parameters up to 3 sigma from mean\n",
    "from Chempy.cem_function import posterior_function_returning_predictions\n",
    "err = []\n",
    "sigma = np.array([0.3,0.3,0.3,0.3,0.1,0.1])\n",
    "Ntest = 500\n",
    "param = np.zeros((Ntest,len(a.p0)))\n",
    "for i in range(Ntest):\n",
    "    print(i)\n",
    "    param[i,:] = a.p0+3*sigma*np.random.rand(6)\n",
    "    rescaled_param = (param[i,:]-a.p0)/np.array(a.training_widths)\n",
    "    hidden1 = np.tanh(np.array(np.dot(w_array_0,rescaled_param)+b_array_0))\n",
    "    neural_output = np.dot(w_array_1, hidden1)+b_array_1\n",
    "    abun,_= posterior_function_returning_predictions((param[i,:],a))\n",
    "    chem_output = abun[element_index]\n",
    "    err.append(np.absolute(chem_output-neural_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(param[:,0],param[:,1],c=err)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New testing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [1,2,3,4,4.3]\n",
    "err_median = []\n",
    "err_up = []\n",
    "err_low = []\n",
    "for width in widths:\n",
    "    tr_abun = np.load('SingleElement/'+str(width)+'_sigma_abundances.npy')[:,element_index]\n",
    "    tr_param = np.load('SingleElement/'+str(width)+'_sigma_param_grid.npy')\n",
    "    \n",
    "    err = []\n",
    "    for i,param in enumerate(tr_param):\n",
    "        neural_abun = neural_pred(param)\n",
    "        err.append(np.absolute(neural_abun-tr_abun[i]))\n",
    "    err_median.append(np.percentile(err,50))\n",
    "    err_up.append(np.percentile(err,100-15.865))\n",
    "    err_low.append(np.percentile(err,15.865))\n",
    "                   \n",
    "errorbar_widths = [np.array(err_median)-np.array(err_low),np.array(err_up)-np.array(err_median)]\n",
    "plt.errorbar(widths,err_median,yerr = errorbar_widths,fmt='-o')\n",
    "plt.ylabel('Network error')\n",
    "plt.xlabel('Width of dataset (sigma)')\n",
    "plt.title('1 element predictions')\n",
    "print(err_up,err_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all elements\n",
    "from Chempy.neural import neural_output\n",
    "widths = [1,2,3,4,4.3]\n",
    "err_median = []\n",
    "err_up = []\n",
    "err_low = []\n",
    "for width in widths:\n",
    "    tr_abun = np.load('SingleElement/'+str(width)+'_sigma_abundances.npy')[:]\n",
    "    tr_param = np.load('SingleElement/'+str(width)+'_sigma_param_grid.npy')\n",
    "    \n",
    "    err = []\n",
    "    for i,param in enumerate(tr_param):\n",
    "        neural_abun = neural_output(param)\n",
    "        temp_err = (np.absolute(neural_abun-tr_abun[i]))\n",
    "        err.append(mean(temp_err))\n",
    "    err_median.append(np.percentile(err,50))\n",
    "    err_up.append(np.percentile(err,100-15.865))\n",
    "    err_low.append(np.percentile(err,15.865))\n",
    "                   \n",
    "errorbar_widths = [np.array(err_median)-np.array(err_low),np.array(err_up)-np.array(err_median)]\n",
    "plt.errorbar(widths,err_median,yerr = errorbar_widths,fmt='-o')\n",
    "plt.ylabel('Mean network error across all elements')\n",
    "plt.xlabel('Width of dataset (sigma)')\n",
    "plt.title('Network for all 28 elements')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing network with 3 sigma ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_3sigma(lr,element_index,neurons=a.neurons):\n",
    "\n",
    "    training_abundances = np.load('Neural/training_abundances.npy')\n",
    "    tr_input = np.load('Neural/training_norm_grid.npy')\n",
    "    tr_output = training_abundances[:,element_index]\n",
    "\n",
    "    dim_in = tr_input.shape[1]\n",
    "    dim_out = 1\n",
    "\n",
    "    tr_input = Variable(torch.from_numpy(tr_input)).type(torch.FloatTensor)\n",
    "    tr_output = Variable(torch.from_numpy(tr_output), requires_grad=False).type(torch.FloatTensor)\n",
    "\n",
    "    model = []\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(dim_in,neurons),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(neurons,dim_out)\n",
    "        )\n",
    "    loss_fn = torch.nn.L1Loss(size_average=True)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    losslog = []\n",
    "    epoch = []\n",
    "\n",
    "    # Train neural network\n",
    "    for i in range(a.epochs):\n",
    "        pred_output = model(tr_input)\n",
    "        loss = loss_fn(pred_output, tr_output)\n",
    "        optimizer.zero_grad() # Initially zero gradient\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update via optimizer\n",
    "\n",
    "        # Output loss\n",
    "        if i % 3 ==0:\n",
    "            losslog.append(loss.data[0])\n",
    "            epoch.append(i)\n",
    "        if i % 500==0:\n",
    "            print(\"Training epoch %d of %d complete\" %(i,a.epochs))\n",
    "\n",
    "    # Convert weights to numpy arrays\n",
    "    model_numpy = []\n",
    "    for param in model.parameters():\n",
    "        model_numpy.append(param.data.numpy())\n",
    "        \n",
    "    w_array_0=model_numpy[0]\n",
    "    b_array_0=model_numpy[1]\n",
    "    w_array_1=model_numpy[2]\n",
    "    b_array_1=model_numpy[3]\n",
    "    \n",
    "    np.save('SingleElement/w0.npy',w_array_0)\n",
    "    np.save('SingleElement/w1.npy',w_array_1)\n",
    "    np.save('SingleElement/b0.npy',b_array_0)\n",
    "    np.save('SingleElement/b1.npy',b_array_1)\n",
    "    \n",
    "    # Test network\n",
    "    all_abun = np.load('SingleElement/3_sigma_abundances.npy')\n",
    "    test_abun = all_abun[:,element_index]\n",
    "    test_param = np.load('SingleElement/3_sigma_param_grid.npy')\n",
    "\n",
    "    err = []\n",
    "    for i,item in enumerate(test_param):\n",
    "        norm_data = (item - a.p0)/np.array(a.training_widths)\n",
    "        hidden1 = np.tanh(np.array(np.dot(w_array_0,norm_data)+b_array_0))\n",
    "        output = np.dot(w_array_1, hidden1)+b_array_1\n",
    "        err.append(abs(output-test_abun[i]))\n",
    "    \n",
    "    err_median =(np.percentile(err,50))\n",
    "    err_up =(np.percentile(err,100-15.865))\n",
    "    err_low =(np.percentile(err,15.865))\n",
    "    return err_median,err_up,err_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.007] # This seems to be best number\n",
    "neurons = [30]\n",
    "err_med = []\n",
    "err_low = []\n",
    "err_up=[]\n",
    "for n in neurons:\n",
    "    print(\"%d neurons testing\" %(n))\n",
    "    med,low,up = single_3sigma(learning_rate[-1],element_index,n)\n",
    "    err_med.append(med)\n",
    "    err_low.append(low)\n",
    "    err_up.append(up)\n",
    "print(err_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorbar_widths = [np.array(err_med)-np.array(err_low),np.array(err_up)-np.array(err_med)]\n",
    "plt.errorbar(neurons,err_med,yerr = errorbar_widths,fmt='o')\n",
    "plt.ylabel('3 sigma network error across all elements')\n",
    "plt.xlabel('No. neurons')\n",
    "plt.title('Error plot for optimization using 3 sigma test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorbar_widths = [np.array(err_med)-np.array(err_low),np.array(err_up)-np.array(err_med)]\n",
    "plt.errorbar(learning_rate,err_med,yerr = errorbar_widths,fmt='o')\n",
    "plt.ylabel('3 sigma network error across all elements')\n",
    "plt.xlabel('Learning rate')\n",
    "plt.title('Error plot for optimization using 3 sigma test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This gives best output for 30 neurons at learning_rate = 0.007. Best output is err = 0.0058+0.0092-0.0047 for the 3sigma test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute best errors from all 28 elements at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Training epoch 0 of 1000 complete\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from Chempy.neural import create_network\n",
    "from Chempy.neural import neural_output\n",
    "\n",
    "# Create neural network\n",
    "\n",
    "create_network(learning_rate=0.007,Plot=False)\n",
    "\n",
    "widths = [1,2,3,4,4.3]\n",
    "err_median = []\n",
    "err_up = []\n",
    "err_low = []\n",
    "for width in widths:\n",
    "    # Use training data of restricted sigma ranges\n",
    "    tr_abun = np.load('SingleElement/'+str(width)+'_sigma_abundances.npy')[:]\n",
    "    tr_param = np.load('SingleElement/'+str(width)+'_sigma_param_grid.npy')\n",
    "    \n",
    "    err = []\n",
    "    for i,param in enumerate(tr_param):\n",
    "        neural_abun = neural_output(param)\n",
    "        temp_err = (np.absolute(neural_abun-tr_abun[i]))\n",
    "        err.append(temp_err)\n",
    "    err_median.append(np.percentile(err,50,axis=0))\n",
    "    err_up.append(np.percentile(err,100-15.865,axis=0))\n",
    "    err_low.append(np.percentile(err,15.865,axis=0))\n",
    "\n",
    "j=element_index # Element index                \n",
    "\n",
    "errorbar_widths = [np.array(err_median)[:,j]-np.array(err_low)[:,j],np.array(err_up)[:,j]-np.array(err_median)[:,j]]\n",
    "plt.errorbar(widths,np.array(err_median)[:,j],yerr = errorbar_widths,fmt='o')\n",
    "plt.ylabel('Mean network error across all elements')\n",
    "plt.xlabel('Width of dataset (sigma)')\n",
    "plt.title('Network for all 28 elements')\n",
    "print(np.array(err_median)[:,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
