
def posterior_function_many_stars_quick(changing_parameter,error_list,error_element_list,preload):
	'''
	This is the actual posterior function for many stars. But the functionality is explained in posterior_function_many_stars.
	This is a cut-down version for efficient mcmc runs
	
	preload input is preloaded parameters from the .wrapper preload_params file
	'''
	## Initialising the model parameters
	a = ModelParameters()

	## extracting from 'changing_parameters' the global parameters and the local parameters
	global_parameters = changing_parameter[:len(a.SSP_parameters)]
	local_parameters = changing_parameter[len(a.SSP_parameters):]
	local_parameters = local_parameters.reshape((len(a.stellar_identifier_list),len(a.ISM_parameters)))

	## getting the prior for the global parameters in order to subtract it in the end for each time it was evaluated too much
	a.to_optimize = a.SSP_parameters_to_optimize
	global_parameter_prior = get_prior(global_parameters,a)
	
	## Chempy is evaluated one after the other for each stellar identifier with the prescribed parameter combination and the element predictions for each star are stored
	predictions_list = []
	elements_list = []
	log_prior_list = []
		
	for i, item in enumerate(a.stellar_identifier_list):
		b = ModelParameters()
		b.stellar_identifier = item
		changing_parameter = np.hstack((global_parameters,local_parameters[i]))
		args = (changing_parameter,b)
		abundance_list,element_list = posterior_function_returning_predictions_quick(changing_parameter,a,preload)
		predictions_list.append(abundance_list)
		elements_list.append(element_list)
		log_prior_list.append(get_prior(changing_parameter,b))

	## The wildcards are read out so that the predictions can be compared with the observations
	args = zip(a.stellar_identifier_list, predictions_list, elements_list)
	list_of_l_input = []
	for item in args:
	    list_of_l_input.append(read_out_wildcard(*item))
	    list_of_l_input[-1] = list(list_of_l_input[-1])

	## Here the predictions and observations are brought into the same array form in order to perform the likelihood calculation fast
	elements = np.unique(np.hstack(elements_list))
	# Masking the elements that are not given for specific stars and preparing the likelihood input
	star_errors = ma.array(np.zeros((len(elements),len(a.stellar_identifier_list))), mask = True)
	star_abundances = ma.array(np.zeros((len(elements),len(a.stellar_identifier_list))), mask = True)
	model_abundances = ma.array(np.zeros((len(elements),len(a.stellar_identifier_list))), mask = True)

	for star_index,item in enumerate(list_of_l_input):
	    for element_index,element in enumerate(item[0]):
	        assert element in elements, 'observed element is not predicted by Chempy'
	        new_element_index = np.where(elements == element)[0][0]
	        star_errors[new_element_index,star_index] = item[1][element_index]
	        model_abundances[new_element_index,star_index] = item[2][element_index]
	        star_abundances[new_element_index,star_index] = item[3][element_index]

	## given model error from error_list is read out and brought into the same element order (compatibility between python 2 and 3 makes the decode method necessary)
	if not a.error_marginalization:
		error_elements_decoded = []
		for item in error_element_list:
			error_elements_decoded.append(item)#.decode('utf8')) # DECODING NOT NEEDED IN PYTHON 3
		error_element_list = np.hstack(error_elements_decoded)


		error_list = np.hstack(error_list)
		model_error = []
		for element in elements:
			assert element in error_element_list, 'for this element the model error was not given, %s' %(element)
			model_error.append(error_list[np.where(error_element_list == element)])
		model_error = np.hstack(model_error)




	## likelihood is calculated (the model error vector is expanded)
	if a.error_marginalization:
		#from scipy.stats import beta
		likelihood_list = []
		model_errors = preload.model_errors
		#model_errors = np.linspace(a.flat_model_error_prior[0],a.flat_model_error_prior[1],a.flat_model_error_prior[2])
		if a.beta_error_distribution[0]:
			#error_weight = beta.pdf(model_errors, a = a.beta_error_distribution[1], b = a.beta_error_distribution[2])
			#error_weight/= sum(error_weight)
			error_weight = preload.error_weight
		else:
			error_weight = np.ones_like(model_errors) * 1./float(flat_model_error_prior[2])
		for i, item in enumerate(model_errors):
			error_temp = np.ones(len(elements)) * item 
			likelihood_list.append(likelihood_evaluation(error_temp[:,None], star_errors , model_abundances, star_abundances))
		likelihood = logsumexp(likelihood_list, b = error_weight)	
	else:
		if a.zero_model_error:
			model_error = np.zeros_like(model_error)
		likelihood = likelihood_evaluation(model_error[:,None], star_errors , model_abundances, star_abundances)
	
	## Prior from all stars is added
	prior = sum(log_prior_list)
	## Prior for global parameters is subtracted
	prior -= (len(a.stellar_identifier_list)-1) * global_parameter_prior
	posterior = prior+likelihood
	assert np.isnan(posterior) == False, ('returned posterior = ', posterior , 'prior = ' , prior, 'likelihood = ', likelihood, 'changing parameter = ', changing_parameter )
	########
	if a.verbose:
		print('prior = ', prior, 'likelihood = ', likelihood)

	return(posterior,model_abundances)
	
def posterior_function_returning_predictions_quick(changing_parameter,a,preload):
	'''
	calls the posterior function but just returns the negative log posterior instead of posterior and blobs. 
	This is a sped up version for use in mcmc
	'''
	## Initialising the model parameters
	a = ModelParameters()


	#start_time = time.time()
	# the values in a are updated according to changing_parameters and the prior list is appended
	a = extract_parameters_and_priors(changing_parameter, a)
	

	# the log prior is calculated
	prior = sum(np.log(a.prior))

	
	#precalculation = time.time()
	#print('precalculation: ', start_time - precalculation)

	# The endtime is changed for the actual calculation but restored to default afterwards
	if not a.UseNeural: # If using neural networks endtime is not changed
		backup = a.end ,a.time_steps, a.total_mass
	
	
	# call Chempy and return the abundances at the end of the simulation = time of star's birth and the corresponding element names as a list
	abundance_list,elements_to_trace = cem_real2(a)
	
	if not a.UseNeural:
		a.end ,a.time_steps, a.total_mass = backup
	
	# The last two entries of the abundance list are the Corona metallicity and the SN-ratio
	list_of_abundances = abundance_list[:-2]
	elements_to_trace = elements_to_trace[:-2]

	#model = time.time()
	#print('model: ', precalculation - model)
	
	# a likelihood is calculated where the model error is optimized analytically if you do not want model error uncomment one line in the likelihood function
	#likelihood, element_list, model_error, star_error_list, abundance_list, star_abundance_list = likelihood_function(a.stellar_identifier, abundance_list, elements_to_trace)
	abundance_list = []
	element_list = []
	
	# Remove this bit from the code
	from . import localpath
	wildcard = np.load(localpath + 'input/stars/' + a.stellar_identifier + '.npy')

	for i,item in enumerate(elements_to_trace):
		if item in list(wildcard.dtype.names):
			element_list.append(item)
			abundance_list.append(float(list_of_abundances[i]))
	#abundance_list = np.hstack(abundance_list)
	
	#likelihood = 0.
	#abundance_list = [0]

	#error_optimization = time.time()
	#print('error optimization: ', model - error_optimization)
	#if a.verbose:
	#	if not a.testing_output:
	#		print('prior = ', prior, 'likelihood = ', likelihood, mp.current_process()._identity[0])
	#	else:
	#		print('prior = ', prior, 'likelihood = ', likelihood)

	return abundance_list, element_list
	
	
class preload_params():
	"""This calculates and stores useful quantities that would be calculated multiple times otherwise.
	Definitions can be called from this file
	
	NB - this only works for one wildcard at the moment - GENERALIZE THIS!!!
	This is old and probably shouldn't be used....
	"""
	from Chempy.parameter import ModelParameters
	import numpy as np
	from scipy.stats import beta
	from Chempy.neural import neural_output
	
	a = ModelParameters()
	
	wildcard = np.load('Chempy/input/stars/'+a.stellar_identifier+'.npy')	
	
	elements = []
	star_abundance_list = []
	star_error_list = []
	for i,item in enumerate(a.elements_to_trace):
		if item in list(wildcard.dtype.names):
			elements.append(item)
			star_abundance_list.append(float(wildcard[item][0]))
			star_error_list.append(float(wildcard[item][1]))
	star_abundance_list = np.hstack(star_abundance_list)
	star_error_list = np.hstack(star_error_list)
	elements = np.hstack(elements)
	
	elements_to_trace = list(a.elements_to_trace)
	elements_to_trace.append('Zcorona')
	elements_to_trace.append('SNratio')
	
	# Neural network coeffs
	coeffs = np.load('Neural/neural_model.npz')
	
	# Beta function errors
	model_errors = np.linspace(a.flat_model_error_prior[0],a.flat_model_error_prior[1],a.flat_model_error_prior[2])
	error_weight = beta.pdf(model_errors, a = a.beta_error_distribution[1], b = a.beta_error_distribution[2])
	error_weight/= sum(error_weight)

	errors_list = []
	for error in model_errors:
		temp_err = np.ones((len(star_error_list),1))*error
		errors_list.append(np.sqrt(np.multiply(temp_err,temp_err) + np.multiply(star_error_list,star_error_list).T).T)
	